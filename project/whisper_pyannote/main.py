"""
Windows 11 + Python 3.11 + PyCharm
Whisper API (whisper-1) + pyannote.audio (speaker-diarization-3.1)
- HF gated ëª¨ë¸: ëª¨ë¸ í˜ì´ì§€ì—ì„œ Access ìˆ˜ë½ í•„ìš”
- íŒŒì´í”„ë¼ì¸ ë¡œë”©: í•­ìƒ MODEL_ID + cache_dir ë°©ì‹
- í™”ì ë¶„ë¦¬ + Whisper ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì„ìŠ¤íƒ¬í”„ ë§¤ì¹­
- ì„¸ê·¸ë¨¼íŠ¸ ì •ê·œí™” í•¨ìˆ˜ë¡œ SDK ë²„ì „ì°¨(ê°ì²´/ë”•ì…”ë„ˆë¦¬) í˜¸í™˜
- (ì˜µì…˜) SRT ì €ì¥
"""

import os
import torch
from pathlib import Path
from typing import List, Dict, Any
from openai import OpenAI
from pyannote.audio import Pipeline as PnPipeline

# ===== ì‚¬ìš©ì ì„¤ì • =====
AUDIO_FILE = Path(r"C:/Users/cdffe/Desktop/vpn/íšŒì˜ë…¹ìŒ.mp3")
SAVE_SRT = True
MODEL_ID = "pyannote/speaker-diarization-3.1"
CACHE_DIR = Path.home()/".cache"/"hf_diarization_cache" # ë‹¤ìš´ë¡œë“œ ìºì‹œ ìœ„ì¹˜
# ======================

def require_env(name: str) -> str:
    v = os.getenv(name)
    if not v:
        raise ValueError(f"{name} í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. PyCharm â†’ Run/Debug Config â†’ Environment variablesì— ì¶”ê°€í•˜ì„¸ìš”.")
    return v

def sec_to_srt(ts: float) -> str:
    if ts is None:
        ts = 0.0
    h = int(ts // 3600)
    ts -= 3600 * h
    m = int(ts // 60)
    ts -= 60 * m
    s = int(ts)
    ms = int(round((ts - s) * 1000))
    return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"

def overlap(a0: float, a1: float, b0: float, b1: float) -> float:
    return max(0.0, min(a1, b1) - max(a0, b0))


#ì„¸ê·¸ë¨¼íŠ¸ ì •ê·œí™” (ê°ì²´/ë”•ì…”ë„ˆë¦¬ ëª¨ë‘ ì§€ì›)
def _seg_to_dict(seg) -> Dict[str, Any]:
    if isinstance(seg, dict):
        start = seg.get("start", 0.0)
        end = seg.get("end", 0.0)
        text = seg.get("text", "")
    else:
        start = getattr(seg, "start", 0.0)
        end = getattr(seg, "end", 0.0)
        text = getattr(seg, "text", "")
    return {"start": float(start or 0.0), "end": float(end or 0.0), "text": str(text or "").strip()}

#whisper ì²˜ë¦¬
def transcribe_with_whisper(api_key: str, audio_path: Path) -> List[Dict[str, Any]]:
    if not audio_path.exists():
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
    print("ğŸ™ Whisper APIë¡œ ìŒì„± ì¸ì‹ ì¤‘...")
    client = OpenAI(api_key=api_key)
    with audio_path.open("rb") as f:
        tr = client.audio.transcriptions.create(
            model="whisper-1", #whipser ëª¨ë¸
            file=f,
            response_format="verbose_json" #segments í¬í•¨
        )

    segments = getattr(tr, "segments", None)
    if segments is None and isinstance(tr, dict):
        segments = tr.get("segments")

    if not segments:
        raise RuntimeError("Whisper ê²°ê³¼ì— segmentsê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸/response_format í™•ì¸.")

    # âœ… ì—¬ê¸°ì„œ ëª¨ë‘ dictë¡œ ì •ê·œí™”í•´ ë°˜í™˜
    return [_seg_to_dict(seg) for seg in segments]


def load_diarization_pipeline(hf_token: str):
    print("ğŸ“¥ í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    print("HF_TOKEN check:", hf_token[:8] + "*" * (len(hf_token) - 8))
    CACHE_DIR.mkdir(parents=True, exist_ok=True)

    # âš ï¸ í•µì‹¬: ì²« ì¸ìëŠ” í•­ìƒ MODEL_ID (ë¡œì»¬ ê²½ë¡œ ê¸ˆì§€), ìºì‹œ ìœ„ì¹˜ë§Œ ì§€ì •
    try:
        diar_pipe = PnPipeline.from_pretrained(
            MODEL_ID,
            use_auth_token=hf_token,
            cache_dir=str(CACHE_DIR)
        )
    except TypeError:
        # ì¼ë¶€ í™˜ê²½ì—ì„œ ì¸ìëª…ì´ auth_tokenì¸ ê²½ìš°
        diar_pipe = PnPipeline.from_pretrained(
            MODEL_ID,
            auth_token=hf_token,
            cache_dir=str(CACHE_DIR)
        )

    if torch.cuda.is_available():
        diar_pipe.to(torch.device("cuda"))  # ì¬í• ë‹¹ ê¸ˆì§€
        print("GPU ì‚¬ìš©: CUDA í™œì„±í™”")
    else:
        print("CPU ì‚¬ìš©")
    return diar_pipe


def match_speakers(segments: List[Dict[str, Any]], diarization) -> List[Dict[str, Any]]:
    print("ğŸ—£ í™”ì ë¶„ë¦¬ ì¤‘...")
    dia = diarization(str(AUDIO_FILE)) #pyannoteëŠ” str ê²½ë¡œ í—ˆìš©
    turns = [(float(t.start), float(t.end), spk) for t, _, spk in dia.itertracks(yield_label=True)]

    out = []
    for seg in segments:
        s0 = float(seg["start"])
        s1 = float(seg["end"])
        txt = seg["text"]

        best_spk, best_ov = "Unknown", 0.0
        for t0, t1, spk in turns:
            ov = overlap(s0, s1, t0, t1)
            if ov > best_ov:
                best_ov, best_spk = ov, spk

        out.append({"start": s0, "end": s1, "speaker": best_spk, "text": txt})
    return out


def save_srt(segments: List[Dict[str, Any]], audio_path: Path) -> Path:
    srt_path = audio_path.with_suffix("").with_name(audio_path.stem + "_diarized.srt")
    with srt_path.open("w", encoding="utf-8") as f:
        for i, seg in enumerate(segments, 1):
            f.write(f"{i}\n")
            f.write(f"{sec_to_srt(seg['start'])} --> {sec_to_srt(seg['end'])}\n")
            f.write(f"{seg['speaker']}: {seg['text']}\n\n")
    return srt_path


def main():
    #í™˜ê²½ë³€ìˆ˜ ì„¸íŒ…
    OPENAI_API_KEY = require_env("OPENAI_API_KEY")
    HF_TOKEN = require_env("HF_TOKEN")

    #ì„¸ê·¸ë¨¼íŠ¸ dict ì •ê·œí™”
    segments = transcribe_with_whisper(OPENAI_API_KEY, AUDIO_FILE)

    #íŒŒì´í”„ë¼ì¸ ë¡œë“œ (MODEL_ID + cache_dir) & ë§¤ì¹­
    diar_pipe = load_diarization_pipeline(HF_TOKEN)
    speakered = match_speakers(segments, diar_pipe)

    print("\ní™”ìë³„ ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸\n" + "-" * 40)
    for seg in speakered:
        print(f"{seg['speaker']}: {seg['text']}")
    print("-" * 40)
    print("ì‘ì—… ì™„ë£Œ")

    #SRT ì €ì¥
    if SAVE_SRT:
        srt = save_srt(speakered, AUDIO_FILE)
        print(f"ğŸ’¾ SRT ì €ì¥: {srt}")


if __name__ == "__main__":
    # ê¶Œì¥ ë²„ì „: numpy==1.26.4, pyannote.audio==3.1.1
    main()
